# -*- coding: utf-8 -*-
"""DataMiningFinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18gTj4vV1sl7HoPlCmFQdpOlum_RpG3q6

# Boarding Patterns in Public Transport

---

 **Esra Kantarcı - 2021**

---
 ## Standard Imports
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install kmodes
!pip install pygal

## for data
import pandas as pd
import numpy as np
from pandas import DataFrame

## for plotting
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams
# %matplotlib inline
import plotly.express as px
import plotly
plotly.offline.init_notebook_mode (connected = True)
from mpl_toolkits.mplot3d import Axes3D

## for statistical tests
import scipy
from scipy.stats import multivariate_normal
import statsmodels.formula.api as smf
import statsmodels.api as sm

## for encoding
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

## for learning
import sklearn

## for clustering
from kmodes.kprototypes import KPrototypes
from kmodes.kmodes import KModes
from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation
from sklearn.mixture import GaussianMixture

## for preprocessing
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler, normalize,MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity

"""# Data Preprocessing"""

df = pd.read_csv('pass.csv')
df.tail()

"""## Line Encoding"""

line_set = set(df['Line'])
line_set

lines = []
for line in line_set:
  selected_data = df.loc[df['Line'] == line]
  lines.append(line)

len(lines)

"""### Line Mapping with Routes: Start - Mid - Terminal

Using antalya-ulasim.com.tr
"""

#Let's create a mapping to add new columns for respective unique lines
#info from antalya-ulasim.com and middle point is heuristically decided

mapped_lines = {'511': "Kundu,Lara Yolu,Doğu Garajı",
 'AC03': "Aksu,100.Yıl,Migros",
 'AF04': "Aksu,Migros,Sarısu",
 'AF04A': "Aksu,Gazi Bulvarı,Uncalı",
 'CV14': "Varsak,Güllük,Işıklar",
 'CV47': "Meydan,Güllük,Migros",
 'CV48': "Varsak,100.Yıl,Güllük",
 'CV67': "Meydan,Meltem,Migros",
 'DC15': "Döşemealtı,Meltem,Meydan",
 'DC15A': "Döşemealtı,Meltem,Meydan",
 'FL82': "Ünsal,Mevlana,Lara",
 'GM24': "Masadağı,Mevlana,Lara",
 'KC06': "Sarısu,Meltem,Muratpaşa",
 'KC33': "Sarısu,Meltem,Konuksever",
 'KC35': "Sarısu,Meltem,Ermenek",
 'KC35A': "Varsak,Meydan,Atatürk Bulvarı",
 'KF52': "Sarısu,Migros,Kepezaltı",
 'KL08': "Sarısu,100.Yıl,Lara",
 'KM61': "Sarısu,Meltem,Masadağı",
 'KPZ83': "Varsak,Güllük,Doğu Garajı",
 'LC07': "Kundu,Markantalya,Güllük",
 'LC07A': "Kundu,Güllük,Özdilek",
 'LF09': "Lara,Meltem,Ünsal",
 'LF10': "Lara,Meltem,Ünsal",
 'MC12': "Masadağı,Güllük,Meydan",
 'MF40': "Masadağı,Meltem,Sarısu",
 'ML22': "Meydan,100.Yıl,Hurma",
 'MZ78': "Kepez,Güllük,Meydan",
 'TB72': "Otogar,Meltem,Baraj Mahallesi",
 'TC16': "Ünsal,Güllük,Ermenek",
 'TC16A': "Ünsal,Güllük,Ermenek",
 'TC93': "Otogar,Güllük,Lara",
 'TCD49': "Ünsal,Markantalya,Kepez",
 'TCD49A': "Ünsal,Markantalya,Kepez",
 'TCP45': "Otogar,Markantalya,Altıayak Mahallesi",
 'TK36': "Otogar,Markantalya,Işıklar",
 'TL94': "Otogar,Güllük,Lara",
 'UC11': "Gürsu,Meltem,Markantalya",
 'UC32': "Sarısu,Meltem,Markantalya",
 'VC53': "Varsak,Güllük,Soğuksu",
 'VC57': "Varsak,Güllük,Liman",
 'VC59': "Varsak,Kepez,Migros",
 'VF01': "Varsak,Markantalya,Akdeniz Üniversitesi",
 'VF02': "Varsak,Otogar,Konyaaltı",
 'VF63': "Varsak,Migros,Jandarma",
 'VF66': "Varsak,Meltem,Güllük",
 'VL13': "Varsak,Mevlana,Lara",
 'VL13A': "Varsak,Mevlana,Lara",
 'VML54': "Varsak,Mevlana,Ermenek",
 'VML55A':"Varsak,Sütçüler,Yalı Caddesi",
 'VS18': "Süleyman Demirel,100.Yıl,Sarısu"}

df['Stops']= df['Line'].map(mapped_lines)

df[['Start','Mid','Terminal']] = df['Stops'].str.split(',',expand=True)
df

"""## Time Encoding"""

#NEAT! Now, let's preprocess the boarding time and then encoding for lines should start.
## 2019-12-18 is a normal wednesday before COVID hit Turkey.

df[['Date', 'Time']] = df['BoardingTime'].str.split('T',expand=True)
df

time_set = set(df['Time'])
time_set

mapped_time = {
    '00:00': 0000,
 '01:00': 100,
 '03:30': 330,
 '05:30': 530,
 '06:00': 600,
 '06:30': 630,
 '07:00': 700,
 '07:30': 730,
 '08:00': 800,
 '08:30': 830,
 '09:00': 900,
 '09:30': 930,
 '10:00': 1000,
 '10:30': 1030,
 '11:00': 1100,
 '11:30': 1130,
 '12:00': 1200,
 '12:30': 1230,
 '13:00': 1300,
 '13:30': 1330,
 '14:00': 1400,
 '14:30': 1430,
 '15:00': 1500,
 '15:30': 1530,
 '16:00': 1600,
 '16:30': 1630,
 '17:00': 1700,
 '17:30': 1730,
 '18:00': 1800,
 '18:30': 1830,
 '19:00': 1900,
 '19:30': 1930,
 '20:00': 2000,
 '20:30': 2030,
 '21:00': 2100,
 '21:30': 2130,
 '22:00': 2200,
 '22:30': 2230,
 '23:00': 2300,
 '23:30': 2330
}

df['EncodedTime']= df['Time'].map(mapped_time)
df

"""# Visualization Before Clustering"""

df_dup = df
df.drop(['BoardingTime', 'Stops', 'Date', 'Time'], axis=1)
columns = ['Start', 'Mid', 'Terminal']
df1 = df[columns]
t_array = df1.to_numpy() 
len(np.unique(t_array))

num_cols=df.select_dtypes(include=['int64']).columns
ctg_cols=df.select_dtypes(include=['object']).columns

print('Numerical Cols=',num_cols)
print('Categorical Cols=',ctg_cols)

"""## EDA"""

cols_val=2
fig, ax = plt.subplots(len(num_cols),cols_val,figsize=(12, 5))
colours_val=['c','b','r','g','y','p','m']
did_not_ran=True
for i,col in enumerate(num_cols):
    for j in range(cols_val):
        if did_not_ran==True:
            sns.boxplot(df[col],ax=ax[i,j],color=colours_val[i+j])
            ax[i,j].set_title(col)
            did_not_ran=False
        else:
            sns.distplot(df[col],ax=ax[i,j],color=colours_val[i+j])
            ax[i,j].set_title(col)
            did_not_ran=True
            
            
plt.suptitle("EDA")
plt.tight_layout()
plt.show()

"""## Scatterplot"""

plt.figure(figsize=(12,5))
sns.scatterplot(df['EncodedTime'] ,df['PassengerCount'])
plt.title('Scatterplot')
plt.show()

## Scaling the numeric values

df1=df[['PassengerCount', 'EncodedTime']]
df1.shape
std=MinMaxScaler()
arr1=std.fit_transform(df1)

"""# Initial Clustering

## K-Means (k=2)
"""

kmeans_cluster=KMeans(n_clusters=2,random_state=7)
result_cluster=kmeans_cluster.fit_predict(arr1)
df1['Clusters']=result_cluster
df1['Clusters'].value_counts()

ax=sns.countplot(x=df1.Clusters)
for index, row in pd.DataFrame(df1['Clusters'].value_counts()).iterrows():
    ax.text(index,row.values[0], str(round(row.values[0])),color='black', ha="center")
    #print(index,row.values[0])
plt.title('Cluster Count')
plt.show()

plt.figure(figsize=(12,5))
sns.scatterplot(x=df1['PassengerCount'],y=df1['EncodedTime'],hue=df1.Clusters,palette="Set2",)
plt.title('2 Clusters')
plt.show()

## Did not like it.

fig,ax=plt.subplots(1,2,figsize=(12,5))
sns.heatmap(df1.loc[df1.Clusters==0,['PassengerCount', 'EncodedTime']].describe().round(),annot=True,fmt='g',ax=ax[0])
ax[0].set_title("Cluster-0")
sns.heatmap(df1.loc[df1.Clusters==1,['PassengerCount', 'EncodedTime']].describe().round(),annot=True,fmt='g',ax=ax[1])
ax[1].set_title("Cluster-1")
plt.suptitle("Cluster Analysis")
plt.show()

"""## Elbow Method for Choosing "k""""

SSE = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, random_state = 7)
    kmeans.fit(arr1)
    SSE.append(kmeans.inertia_)

plt.figure(figsize=(12,5))
sns.lineplot(range(1, 11), SSE,marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.show()

"""## K-Means (k=4)"""

kmeans_cluster=KMeans(n_clusters=4,random_state=7)
result_cluster=kmeans_cluster.fit_predict(arr1)

df1['Clusters']=result_cluster
df1['Clusters'].value_counts()

d1=df[['Line','Start','Mid','Terminal']].reset_index(drop=True)
df1_comb=pd.concat([df1.reset_index(drop=True),d1],axis=1)
df1_comb.head()

ax=sns.countplot(x=df1_comb.Clusters)
for index, row in pd.DataFrame(df1_comb['Clusters'].value_counts()).iterrows():
    ax.text(index,row.values[0], str(round(row.values[0])),color='black', ha="center")
    #print(index,row.values[0])
plt.title('Cluster Count')
plt.show()

plt.figure(figsize=(12,7))
sns.scatterplot(x=df1_comb['PassengerCount'],y=df1_comb['EncodedTime'],hue=df1_comb.Clusters,palette="Set2",)
plt.title('4 Clusters')
plt.show()

fig,ax=plt.subplots(1,4,figsize=(15,5))
#cbar_ax = fig.add_axes([1.03, .3, .03, .4])
for cluster_val in sorted(df1_comb.Clusters.unique()):
    #print(cluster_val)
    sns.heatmap(df1_comb.loc[df1_comb.Clusters==cluster_val,['PassengerCount', 'EncodedTime']].describe().round(),annot=True,fmt='g',ax=ax[cluster_val],\
               cbar=i == 0,vmin=0, vmax=130)
    titl='Cluster-'+str(cluster_val)
    ax[cluster_val].set_title(titl)
    
plt.suptitle('Clustering Analysis')

#plt.tight_layout()
plt.show()

fig,ax=plt.subplots(1,4,figsize=(80,25))

for cluster_val in sorted(df1_comb.Clusters.unique()):
    #print(cluster_val)
    sns.heatmap(df1_comb.loc[df1_comb.Clusters==cluster_val,:].groupby('Line').agg({'Clusters':'size','PassengerCount':'mean','EncodedTime':'mean'}).\
    rename(columns={'Clusters':'Count','PassengerCount':'CountMean','EncodedTime':'TimeMean'})\
                .fillna(0).round(),annot=True,fmt='g',ax=ax[cluster_val],cbar=i == 0,vmin=0, vmax=130)
    titl='Cluster-'+str(cluster_val)+' Analysis'
    ax[cluster_val].set_title(titl)
    

plt.suptitle('Clustering Line wise Analysis')
plt.show()

"""## K-Prototypes (k=2)"""

df_proto=pd.DataFrame(arr1,columns=['PassengerCount','EncodedTime'])
df_proto

d2=pd.concat([df_proto,d1],axis=1)
le = preprocessing.LabelEncoder()
df_transformed = df.apply(le.fit_transform)
df_transformed.head()

km_huang = KModes(n_clusters=2, init = "Huang", n_init = 1, verbose=1)
fitClusters_huang = km_huang.fit_predict(df_transformed)

"""### Cost"""

cost = []
for num_clusters in list(range(1,5)):
    kmode = KModes(n_clusters=num_clusters, init = "Cao", n_init = 1, verbose=1)
    kmode.fit_predict(df_transformed)
    cost.append(kmode.cost_)

y = np.array([i for i in range(1,5,1)])
plt.plot(y,cost)

"""# Visualization for Further Evaluation"""

df_cat= df.select_dtypes(include=['object']).copy()
df_cat2 = df_cat.drop(["BoardingTime", "Stops","Date","Time"], axis=1)
print(df_cat2['Line'].value_counts())

"""## Frequency Distribution of Lines"""

line_count = df_cat['Line'].value_counts()

sns.set(rc={'figure.figsize':(30,8)})
sns.set(style="darkgrid")
sns.barplot(line_count.index, line_count.values, alpha=0.9)
plt.title('Frequency Distribution of Lines')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Line', fontsize=12)
plt.show()

"""## Frequency Distribution of Start"""

line_count = df_cat['Start'].value_counts()

sns.set(rc={'figure.figsize':(30,8)})
sns.set(style="darkgrid")
sns.barplot(line_count.index, line_count.values, alpha=0.9)
plt.title('Frequency Distribution of Start')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Line', fontsize=12)
plt.show()

"""## Frequency Distribution of Mid"""

line_count = df_cat['Mid'].value_counts()

sns.set(rc={'figure.figsize':(30,8)})
sns.set(style="darkgrid")
sns.barplot(line_count.index, line_count.values, alpha=0.9)
plt.title('Frequency Distribution of Middle Stop')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Line', fontsize=12)
plt.show()

"""## Frequency Distribution of Terminal"""

line_count = df_cat['Terminal'].value_counts()

sns.set(rc={'figure.figsize':(30,8)})
sns.set(style="darkgrid")
sns.barplot(line_count.index, line_count.values, alpha=0.9)
plt.title('Frequency Distribution of Terminal')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Line', fontsize=12)
plt.show()

"""# Data Preprocessing for Coordinates of the Stops"""

np.unique(t_array)

"""## Latitudes - Longitudes Dictionary Mapping"""

#coordinates from Google Earth

dict_direction = {
    '100.Yıl': "36.88091828970608, 30.653731398108008", 
    'Akdeniz Üniversitesi': "36.89467934489337, 30.655363726943822", 
    'Aksu': "36.96021770075918, 30.84575139810942",
     'Altıayak Mahallesi' : "36.98368888544924, 30.748494813452496",
       'Atatürk Bulvarı': "36.86193699532904, 30.628447584614626",
        'Baraj Mahallesi': "36.94162337903039, 30.747150660614448",
         'Doğu Garajı': "36.89001169400318, 30.71165533730258", 
         'Döşemealtı': "37.02584375233796, 30.599531013453312",
       'Ermenek': "36.87240703924288, 30.831661097188913",
        'Gazi Bulvarı': "36.91356897785544, 30.72328832694419", 
        'Güllük': "36.88617037363356, 30.696885598107983", 
        'Gürsu': "36.86209731092899, 30.632476301806758", 
        'Hurma': "36.857691232370584, 30.608459034420836", 
        'Işıklar': "36.87855617899591, 30.710220198107898",
       'Jandarma': "36.8985930590109, 30.667138713450925", 
       'Kepez': "36.91155948946759, 30.677906257577988", 
       'Kepezaltı': "36.93847298727586, 30.652635755780366", 
       'Konuksever': "36.91118935617863, 30.704526159704034", 
       'Konyaaltı': "36.8626721060457, 30.637890926943257",
       'Kundu': "36.858952583137096, 30.867022660043464", 
       'Lara': "36.84818298806696, 30.816118692010278", 
       'Lara Yolu': "36.851936237180496, 30.749744926943052", 
       'Liman': "36.8333944331254, 30.597597656446812", 
       'Markantalya' : "36.89298695059131, 30.704304111601154", 
       'Masadağı' : "36.951643298787566, 30.679494665870145",
       'Meltem': "36.8927977510592, 30.67018119810819", 
       'Mevlana': "36.89241991592911, 30.716316784615135", 
       'Meydan': "36.88154786974134, 30.72871252204114", 
       'Migros': "36.882991048844254, 30.659578540436637", 
       'Muratpaşa': "36.86910774916523, 30.7366122685326", 
       'Otogar': "36.92149037091927, 30.666318255779974",
       'Sarısu': "36.84373016846354, 30.5971987481215", 
       'Soğuksu': "36.89727024490878, 30.680198093295306", 
       'Süleyman Demirel': "36.952078813588315, 30.70802586927359", 
       'Sütçüler': "36.9339376148595, 30.71224877326034", 
       'Uncalı': "36.88290582857809, 30.628253767422727",
       'Varsak': "36.982122038557264, 30.71401115595764", 
       'Yalı Caddesi': "36.86783887941553, 30.752311042285868", 
       'Özdilek': "36.910288293600146, 30.6780370711224", 
       'Ünsal': "36.93094360302127, 30.640303412003576"
}

df_sui = df_cat2
df_sui['EncodedStart']= df['Start'].map(dict_direction)
df_sui['EncodedMid']= df['Mid'].map(dict_direction)
df_sui['EncodedTerminal']= df['Terminal'].map(dict_direction)

df_sui

df_sui[['Start_X','Start_Y']] = df_sui['EncodedStart'].str.split(',',expand=True)
df_sui[['Mid_X','Mid_Y']] = df_sui['EncodedMid'].str.split(',',expand=True)
df_sui[['Terminal_X','Terminal_Y']] = df_sui['EncodedTerminal'].str.split(',',expand=True)
df_sui

df_sui.dtypes

df_last = pd.DataFrame(df, columns = ['PassengerCount'])

df_last["Start_X"] = df_sui["Start_X"]
df_last["Start_Y"] = df_sui["Start_Y"]
df_last["Mid_X"] = df_sui["Mid_X"]
df_last["Mid_Y"] = df_sui["Mid_Y"]
df_last["Terminal_X"] = df_sui["Terminal_X"]
df_last["Terminal_Y"] = df_sui["Terminal_Y"]
df_last

"""## Scaling"""

x = df_last.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_scaled = pd.DataFrame(x_scaled)

df_scaled["PassengerCount"] = df["PassengerCount"]
df_scaled["EncodedTime"] = df["EncodedTime"]

df_scaled

"""# Clustering Analysis

## K-Means with Coordinates Applied (k=5)
"""

df_scaled

# ref: https://www.kaggle.com/dhanyajothimani/basic-visualization-and-clustering-in-python

def doKmeans(X, nclust=2):
    model = KMeans(nclust)
    model.fit(X)
    clust_labels = model.predict(X)
    cent = model.cluster_centers_
    return (clust_labels, cent)

clust_labels, cent = doKmeans(df_scaled, 5)
kmeans = pd.DataFrame(clust_labels)
df_scaled.insert((df_scaled.shape[1]),'kmeans',kmeans)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_scaled['PassengerCount'],df_scaled['EncodedTime'],
                     c=kmeans[0],s=50)
ax.set_title('K-Means Clustering k=5')
ax.set_xlabel('Count')
ax.set_ylabel('Time')
plt.colorbar(scatter)

"""## Agglomerative Clustering Applied (n=5)"""

def doAgglomerative(X, nclust=2):
    model = AgglomerativeClustering(n_clusters=nclust, affinity = 'euclidean', linkage = 'ward')
    clust_labels1 = model.fit_predict(X)
    return (clust_labels1)

clust_labels1 = doAgglomerative(df_scaled, 5)
agglomerative = pd.DataFrame(clust_labels1)
df_scaled.insert((df_scaled.shape[1]),'agglomerative',agglomerative)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_scaled['PassengerCount'],df_scaled['EncodedTime'],
                     c=agglomerative[0],s=50)
ax.set_title('Agglomerative Clustering')
ax.set_xlabel('Count')
ax.set_ylabel('Time')
plt.colorbar(scatter)

"""## Gaussian Mixture Clustering Applied (n=5)"""

def doGMM(X, nclust=2):
    model = GaussianMixture(n_components=nclust,init_params='kmeans')
    model.fit(X)
    clust_labels3 = model.predict(X)
    return (clust_labels3)

clust_labels3 = doGMM(df_scaled,5)
gmm = pd.DataFrame(clust_labels3)
df_scaled.insert((df_scaled.shape[1]),'gmm',gmm)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_scaled['PassengerCount'],df_scaled['EncodedTime'],
                     c=gmm[0],s=50)
ax.set_title('Gaussian Mixture Clustering')
ax.set_xlabel('Count')
ax.set_ylabel('Time')
plt.colorbar(scatter)

"""## K-Means Elbow"""

n_clusters=30
cost=[]
for i in range(1,n_clusters):
    kmean= KMeans(i)
    kmean.fit(df_scaled)
    cost.append(kmean.inertia_)

plt.plot(cost, 'bx-')

## 3 seems a better choice

"""## K-Means (k=3)"""

kmean= KMeans(3)
kmean.fit(df_scaled)
labels=kmean.labels_

clusters=pd.concat([df_scaled, pd.DataFrame({'cluster':labels})], axis=1)
clusters.head()

"""# Visualization of the Results"""

for c in clusters:
    grid= sns.FacetGrid(clusters, col='cluster')
    grid.map(plt.hist, c)

df_PCA = pd.DataFrame(df_dup, columns = ['EncodedTime', 'PassengerCount'])
df_PCA.shape

dist = 1 - cosine_similarity(df_PCA)

pca = PCA(2)
pca.fit(dist)
X_PCA = pca.transform(dist)
X_PCA.shape

x, y = X_PCA[:, 0], X_PCA[:, 1]

colors = {0: 'red',
          1: 'blue',
          2: 'green',
          3: 'yellow'}

names = {0: 'Night', 
         1: 'Morning', 
         2: 'After Noon'}

df = pd.DataFrame({'x': x, 'y':y, 'label':labels}) 
groups = df.groupby('label')

fig, ax = plt.subplots(figsize=(20, 13)) 

for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,
            color=colors[name],label=names[name], mec='none')
    ax.set_aspect('auto')
    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')
    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')
    
ax.legend()
ax.set_title("Population Segmentation based on Time/Activity")
plt.show()

df_scaled["Line"] = df_dup["Line"]
df_scaled.groupby("gmm")["PassengerCount"].sum()

"""## Popular lines"""

rcParams['figure.figsize'] = 30,8.27

sns.catplot(x="PassengerCount", y="Line", data=df_scaled,height=12, aspect=7/4)

"""## Bus Schedule Visualization"""

sns.catplot(x="EncodedTime", y="Line", data=df_scaled,height=12, aspect=7/4)

"""## Time Groupings"""

sns.catplot(x="EncodedTime", y="PassengerCount", kind="box", data=df_scaled,height=12, aspect=7/4)

"""## PCA Attempt"""

X = df_scaled.drop(["Line"], axis=1)
pca = PCA(n_components=2)
pca.fit(X)

print(pca.components_)
print(pca.explained_variance_)

pca = PCA(n_components=1)
pca.fit(X)
X_pca = pca.transform(X)
print("original shape:   ", X.shape)
print("transformed shape:", X_pca.shape)

X_new = pca.inverse_transform(X_pca)
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], alpha=0.2)
plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)
plt.axis('equal');

"""## Antalya's Popular Stops By Coordinates"""

X = X.rename(columns={0: "Start_X", 1: "Start_Y",2: "Mid_X", 3: "Mid_Y",4: "Terminal_X", 5: "Terminal_Y"})

fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')

ax.scatter(X["Mid_X"],X["Mid_Y"],X["PassengerCount"])

ax.set_title("Middle Stop Coordinates and Passenger Count")

ax.set_xlabel("Latitudes of Stop")

ax.set_ylabel("Longitudes of Stop")

ax.set_zlabel("Passenger Count")


plt.show()

fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')

ax.scatter(X["Mid_X"],X["EncodedTime"],X["PassengerCount"], marker = "x", c="red")

ax.set_title("Middle Stop Latitude Coordinates, Time and Passenger Count")

ax.set_ylabel("Time")

ax.set_xlabel("Latitudes of Stop")

ax.set_zlabel("Passenger Count")


plt.show()

fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')

ax.scatter(X["Mid_Y"],X["EncodedTime"],X["PassengerCount"], marker = "o", c="red")

ax.set_title("Middle Stop Longitude Coordinates, Time and Passenger Count")

ax.set_ylabel("Time")

ax.set_xlabel("Longitudes of Stop")

ax.set_zlabel("Passenger Count")


plt.show()

fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')

ax.scatter(X["Mid_X"],X["Mid_Y"],X["EncodedTime"], marker = "x", c="blue")

ax.set_title("Middle Stop Coordinates and Encoded Time")

ax.set_ylabel("Y")

ax.set_xlabel("X")

ax.set_zlabel("Time")


plt.show()

df = pd.DataFrame(df_sui, columns = ['Line', 'Start_X', 'Start_Y','Terminal_X', 'Terminal_Y'])
df["PassengerCount"] = df_last["PassengerCount"]
df["EncodedTime"] = df_dup["EncodedTime"]
df

"""## Correlation Heatmap"""

df["Start_X"] = pd.to_numeric(df["Start_X"])
df["Start_Y"] = pd.to_numeric(df["Start_Y"])
df["Terminal_X"] = pd.to_numeric(df["Terminal_X"])
df["Terminal_Y"] = pd.to_numeric(df["Terminal_Y"])

def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(14, 12))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        df.corr(), 
        cmap = colormap,
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
    )
    
    plt.title('Pearson Correlation of Coordinates, Count and Time', y=1.05, size=15)

correlation_heatmap(df)

"""# Distance - Line - Coordinate Popularity

## Data Preprocessing by grouping
"""

line_count = df.groupby('Line')['PassengerCount'].sum()

time_count = df.groupby('EncodedTime')['PassengerCount'].sum()

#Not helpful
df.groupby('EncodedTime')['Line'].value_counts()

df_line_count = line_count.to_frame()
df_time_count = time_count.to_frame()

def distance(start_x, start_y, terminal_x, terminal_y):
  d = ((terminal_x - start_x)**2 + (terminal_y - start_y)**2)**0.5
  return d

## Test:
print(distance(0,0,3,4))

encoded_time_set = df['EncodedTime'].unique()
encoded_time_set

df["PassengerCount"].sum()

line_distance = []

for line in line_set:
  selected_data = df.loc[df['Line'] == line]
  x1 = float(selected_data.iloc[-1].Start_X)
  y1 = float(selected_data.iloc[-1].Start_Y)
  x2 = float(selected_data.iloc[-1].Terminal_X)
  y2 = float(selected_data.iloc[-1].Terminal_Y)
  dist = distance(x1,y1,x2,y2)
  line_distance_comb = line + "-" + str(dist*1000)
  line_distance.append(line_distance_comb)

line_distance

df_distance = DataFrame (line_distance,columns=['Line-Distance'])

"""### Line-Distance Preprocessing"""

df_distance[['Line','Distance']] = df_distance['Line-Distance'].str.split('-',expand=True)
df_distance = df_distance.drop(["Line-Distance"], axis=1)
df_distance

df_line_count

merged_df = pd.merge(df_line_count, df_distance, on="Line")
merged_df["PassengerCount"] = pd.to_numeric(merged_df["PassengerCount"])
merged_df["Distance"] = pd.to_numeric(merged_df["Distance"])
merged_df

"""## Pearson Correlation of Distance - Popularity"""

def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(7, 6))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        merged_df.corr(), 
        cmap = colormap,
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
    )
    
    plt.title('Pearson Correlation of Features', y=1.05, size=15)

correlation_heatmap(merged_df)

df_last.dtypes

"""## Pearson Correlation of Coordinates - Distance - Popularity"""

df_last["Start_X"] = pd.to_numeric(df_last["Start_X"])
df_last["Start_Y"] = pd.to_numeric(df_last["Start_Y"])
df_last["Mid_X"] = pd.to_numeric(df_last["Mid_X"])
df_last["Mid_Y"] = pd.to_numeric(df_last["Mid_Y"])
df_last["Terminal_X"] = pd.to_numeric(df_last["Terminal_X"])
df_last["Terminal_Y"] = pd.to_numeric(df_last["Terminal_Y"])

def correlation_heatmap(df):
    _ , ax = plt.subplots(figsize =(7, 6))
    colormap = sns.diverging_palette(220, 10, as_cmap = True)
    
    _ = sns.heatmap(
        df.corr(), 
        cmap = colormap,
        square=True, 
        cbar_kws={'shrink':.9 }, 
        ax=ax,
        annot=True, 
        linewidths=0.1,vmax=1.0, linecolor='white',
    )
    
    plt.title('Pearson Correlation of Features', y=1.05, size=15)

correlation_heatmap(df_last)

df_line_encoded = DataFrame (merged_df)

"""### Line - Coordinates Preprocessing"""

df_line_encoded['Stops']= df['Line'].map(mapped_lines)
df_line_encoded[['Start','Mid','Terminal']] = df_line_encoded['Stops'].str.split(',',expand=True)

df_line_encoded['EncodedStart']= df_line_encoded['Start'].map(dict_direction)
df_line_encoded['EncodedMid']= df_line_encoded['Mid'].map(dict_direction)
df_line_encoded['EncodedTerminal']= df_line_encoded['Terminal'].map(dict_direction)

df_line_encoded[['Start_X','Start_Y']] = df_line_encoded['EncodedStart'].str.split(',',expand=True)
df_line_encoded[['Mid_X','Mid_Y']] = df_line_encoded['EncodedMid'].str.split(',',expand=True)
df_line_encoded[['Terminal_X','Terminal_Y']] = df_line_encoded['EncodedTerminal'].str.split(',',expand=True)

df_line_encoded["Start_X"] = pd.to_numeric(df_line_encoded["Start_X"])
df_line_encoded["Start_Y"] = pd.to_numeric(df_line_encoded["Start_Y"])
df_line_encoded["Mid_X"] = pd.to_numeric(df_line_encoded["Mid_X"])
df_line_encoded["Mid_Y"] = pd.to_numeric(df_line_encoded["Mid_Y"])
df_line_encoded["Terminal_X"] = pd.to_numeric(df_line_encoded["Terminal_X"])
df_line_encoded["Terminal_Y"] = pd.to_numeric(df_line_encoded["Terminal_Y"])

df_line_encoded

"""### Pearson Correlation Heatmap for Distance-Coordinates-PassengerCount"""

correlation_heatmap(df_line_encoded)

df_line_encoded = df_line_encoded.drop(["Stops", "EncodedStart","EncodedMid","EncodedTerminal"], axis=1)

df_line_num = df_line_encoded.drop(["Line", "Start", "Mid", "Terminal"], axis=1)
df_line_num

"""## K-Means Clustering Applied (k=4)"""

clust_labels, cent = doKmeans(df_line_num, 4)
kmeans = pd.DataFrame(clust_labels)
df_line_num.insert((df_line_num.shape[1]),'kmeans',kmeans)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_line_num['Distance'],df_line_num['PassengerCount'],
                     c=kmeans[0],s=50)
ax.set_title('K-Means Clustering')
ax.set_xlabel('Distance')
ax.set_ylabel('PassengerCount')
plt.colorbar(scatter)

clust_labels, cent = doKmeans(df_line_num, 4)
kmeans = pd.DataFrame(clust_labels)
df_line_num.insert((df_line_num.shape[1]),'kmeans_start',kmeans)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_line_num['Start_X'],df_line_num['PassengerCount'],
                     c=kmeans[0],s=50)
ax.set_title('K-Means Clustering with k=4')
ax.set_xlabel('Start_X')
ax.set_ylabel('PassengerCount')
plt.colorbar(scatter)

arr1=std.fit_transform(df_line_num)
result_cluster=kmeans_cluster.fit_predict(arr1)
df_line_num['Clusters']=result_cluster
df_line_num['Clusters'].value_counts()

SSE = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, random_state = 7)
    kmeans.fit(arr1)
    SSE.append(kmeans.inertia_)

plt.figure(figsize=(12,5))
sns.lineplot(range(1, 11), SSE,marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.show()

"""K-Means Clustering Applied (k=2)"""

clust_labels, cent = doKmeans(df_line_num, 2)
kmeans = pd.DataFrame(clust_labels)
df_line_num.insert((df_line_num.shape[1]),'kmeans_start_k2',kmeans)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_line_num['Start_X'],df_line_num['PassengerCount'],
                     c=kmeans[0],s=50)
ax.set_title('K-Means Clustering with k=2')
ax.set_xlabel('Start_X')
ax.set_ylabel('PassengerCount')
plt.colorbar(scatter)

"""## K-Means Clustering Applied (k=3)"""

clust_labels, cent = doKmeans(df_line_num, 3)
kmeans = pd.DataFrame(clust_labels)
df_line_num.insert((df_line_num.shape[1]),'kmeans_k3',kmeans)

fig = plt.figure()
ax = fig.add_subplot(111)
scatter = ax.scatter(df_line_num['Distance'],df_line_num['PassengerCount'],
                     c=kmeans[0],s=50)
ax.set_title('K-Means Clustering k=3')
ax.set_xlabel('Distance')
ax.set_ylabel('PassengerCount')
plt.colorbar(scatter)

"""## BIRCH on Line"""

from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import Birch

dataset = df_line_num
  
# Generating 600 samples using make_blobs
dataset, clusters = make_blobs(n_samples = 51, centers = 8, cluster_std = 0.75, random_state = 0)
  
# Creating the BIRCH clustering model
model = Birch(branching_factor = 50, n_clusters = None, threshold = 1.5)
  
# Fit the data (Training)
model.fit(dataset)
  
# Predict the same data
pred = model.predict(dataset)
  
# Creating a scatter plot
plt.scatter(dataset[:, 0], dataset[:, 1], c = pred, cmap = 'rainbow', alpha = 0.7, edgecolors = 'b')
plt.show()

df_line_num.describe

"""The results will be given in the article."""

df_line_count

df_merge_birch = df_line_encoded
df_merge_birch["birch"] = pd.DataFrame(pred)

df_merge_birch

df_merge_birch.groupby('birch')['Line'].value_counts()

df_merge_birch.groupby('birch')['Mid'].value_counts()

df_merge_birch.groupby('Mid')['birch'].value_counts()

"""## BIRCH on Time"""

data = [df_scaled["PassengerCount"], df_scaled["EncodedTime"]]
headers = ["PassengerCount", "EncodedTime"]
df_time = pd.concat(data, axis=1, keys=headers)

df_time

dataset = df_time

dataset, clusters = make_blobs(n_samples = len(df_time), centers = 8, cluster_std = 0.75, random_state = 0)
  
# Creating the BIRCH clustering model
model = Birch(branching_factor = 50, n_clusters = None, threshold = 1.5)
  
# Fit the data (Training)
model.fit(dataset)
  
# Predict the same data
pred = model.predict(dataset)
  
# Creating a scatter plot
plt.scatter(dataset[:, 0], dataset[:, 1], c = pred, cmap = 'rainbow', alpha = 0.7, edgecolors = 'b')
plt.show()

df_time_birch = df_time
df_time_birch["birch"] = pd.DataFrame(pred) 
df_time_birch

df_time_birch.groupby('birch')['EncodedTime'].value_counts()

sns.catplot(x="birch", y="EncodedTime", data=df_time_birch,height=10, aspect=6/4)

sns.set_style("whitegrid", {'axes.grid' : False})

fig = plt.figure(figsize=(6,6))

ax = Axes3D(fig) # Method 1
# ax = fig.add_subplot(111, projection='3d') # Method 2

z = df_time_birch["birch"]
x = df_time_birch["EncodedTime"]
y = df_time_birch["PassengerCount"]


ax.scatter(x, y, z, c=x, marker='o')
ax.set_zlabel('birch')
ax.set_xlabel('time')
ax.set_ylabel('count')

plt.show()

#Seaborn pair plot
df_3d = pd.DataFrame()
df_3d['time'] = x
df_3d['count'] = y
df_3d['birch'] = z

sns.pairplot(df_3d, hue='birch')

"""# Clustering by Count

## Visualization of Distributions Before Clustering
"""

df["Mid_X"] = df_last["Mid_X"]
df["Mid_Y"] = df_last['Mid_Y']

plt.style.use('fivethirtyeight')

plt.figure(1 , figsize = (10 , 30))
n = 0 
for x in ['PassengerCount' , 'EncodedTime' , 'Start_X', 'Start_Y', 'Mid_X', 'Mid_Y', 'Terminal_X', 'Terminal_Y']:
    n += 1
    plt.subplot(4 , 2 , n)
    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)
    sns.distplot(df[x] , bins = 20)
    plt.title('Distplot of {}'.format(x))
plt.show()

"""## Elbow Method"""



X1 = df[['PassengerCount' , 'EncodedTime']].iloc[: , :].values
inertia = []
for n in range(1 , 11):
    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm='elkan') )
    algorithm.fit(X1)
    inertia.append(algorithm.inertia_)

## Elbow Method
plt.figure(1 , figsize = (15 ,6))
plt.plot(np.arange(1 , 11) , inertia , 'o')
plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
plt.show()

"""Segmentation analysis on SNS plot failed by the RAM.

## K-Means on Time-PassengerCount Applied (k=5)
### Segmentation of the time-clusters
"""

x = df[['PassengerCount' , 'EncodedTime']].iloc[: , :].values

km = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_means = km.fit_predict(x)

plt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'cyan', label = 'Evening(18:30 - 23:30)')
plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'Shift Ends(15:00 - 18:00)')
plt.scatter(x[y_means == 3, 0], x[y_means == 3, 1], s = 100, c = 'magenta', label = 'Noon(10:30 - 14:30)')
plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'yellow', label = 'Shift Starts(06:30 - 10:00')
plt.scatter(x[y_means == 4, 0], x[y_means == 4, 1], s = 100, c = 'orange', label = 'Night(00:00 - 06:00')
plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'Centroids of Clusters')

plt.style.use('fivethirtyeight')
plt.title('K Means Clustering k=5', fontsize = 20)
plt.xlabel('Time')
plt.ylabel('PassengerCount')
plt.legend()
plt.grid()
plt.show()

"""## Dendrogram of PassengerCount and EncodedTime"""

df

import scipy.cluster.hierarchy as sch
x = df.iloc[:, [5, 6]].values
x

dendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))
plt.title('Dendrogam', fontsize = 20)
plt.xlabel('Time')
plt.ylabel('PassengerCount Euclidian Distance')
plt.show()



## Nope. Did not like it.

"""# Line and Behavior Analysis
---


MCA on Line - Time analysis 

---
"""

!pip install prince 

import prince # for multiple correspondence analysis
from sklearn.feature_selection import SelectKBest, chi2 # for chi-squared feature selection
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

df

mca = prince.MCA(
        n_components=2,
        n_iter=3,
        copy=True,
        check_input=True,
        engine='auto',
        random_state=42
        )
df_mca = mca.fit(df)

ax = df_mca.plot_coordinates(
        X=df,
        ax=None,
        figsize=(8, 10),
        show_row_points=False,
        row_points_size=0,
        show_row_labels=False,
        show_column_points=True,
        column_points_size=30,
        show_column_labels=True,
        legend_n_cols=1
               ).legend(loc='center left', bbox_to_anchor=(1, 0.5))

df

df_time_birch["Line"] = df["Line"]
df_time_birch

df_time_birch["birch"].unique()

"""Didn't like BIRCH, it does not end up as I expected

## BIRCH Visualization for Lines
"""

y = df_time_birch["EncodedTime"]
z = df_time_birch["PassengerCount"]
n = df_time_birch["Line"]

plt.rcParams["figure.figsize"] = (20,50)

colors = {0: 'yellowgreen',1: 'cyan',2: 'magenta',3:'red',4:'blue',5:'black',6: 'yellow',7: 'green'}
fig, ax = plt.subplots()
ax.scatter(y, z, c=df_time_birch['birch'].map(colors))
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], z[i]))

df_scaled.groupby("gmm")["PassengerCount"].sum()

df_scaled

"""## Agglomerative Clustering Visualization for Lines"""

y = df_scaled["EncodedTime"]
z = df_scaled["PassengerCount"]
n = df_scaled["Line"]

plt.rcParams["figure.figsize"] = (20,50)

colors = {0: 'yellowgreen',1: 'cyan',2: 'magenta',3:'red',4:'blue',5:'black',6: 'yellow',7: 'green'}
fig, ax = plt.subplots()
ax.scatter(y, z, c=df_scaled['agglomerative'].map(colors))
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], z[i]))

df_scaled

"""### K-Prototypes Clustering Visualization for Lines"""

km_huang = KModes(n_clusters=12, init = "Huang", n_init = 1, verbose=1)
fitClusters_huang = km_huang.fit_predict(df_time)

df_time["kmode"] = fitClusters_huang

df_time["kmode"].unique()

df_time["kmode"] = fitClusters_huang
y = df_time["EncodedTime"]
z = df_time["PassengerCount"]
n = df_time["Line"]

plt.rcParams["figure.figsize"] = (20,50)

colors = {0: 'yellowgreen',1: 'cyan',2: 'magenta',3:'red',4:'blue',5:'black',6: 'yellow',7: 'green', 8: "pink", 9: "purple", 10: "white", 11: "grey"}
fig, ax = plt.subplots()
ax.scatter(y, z, c=df_time['kmode'].map(colors))
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], z[i]))

## Can't see the pattern

df_huang_0 =  df_time[df_time.kmode == 0]
df_huang_1 =  df_time[df_time.kmode == 1]
df_huang_2 =  df_time[df_time.kmode == 2]
df_huang_3 =  df_time[df_time.kmode == 3]
df_huang_4 =  df_time[df_time.kmode == 4]
df_huang_5 =  df_time[df_time.kmode == 5]
df_huang_6 =  df_time[df_time.kmode == 6]

df_huang_0.describe()

df_huangs = [df_huang_0,df_huang_1,df_huang_2,df_huang_3,df_huang_4,df_huang_5,df_huang_6]
count = 0
for x in df_huangs:
  print("-----")
  print("DF_HUANG_" + str(count))
  print(x.groupby("EncodedTime")["Line"].count())
  print(x.describe())
  count = count+1

### So... it ended up as time based clustering, again...

df_sui

df_sui["EncodedTime"] = df["EncodedTime"]
df_sui["PassengerCount"] = df["PassengerCount"]

df_sui

"""### K-Prototypes Clustering for Lines (With PassengerCount > 300) """

km_huang = KModes(n_clusters=7, init = "Huang", n_init = 1, verbose=1)
fitClusters_huang = km_huang.fit_predict(df_sui)

df_sui["kmode"] = fitClusters_huang
y = df_sui["EncodedTime"]
z = df_sui["PassengerCount"]
n = df_sui["Line"]

plt.rcParams["figure.figsize"] = (20,50)

colors = {0: 'yellowgreen',1: 'cyan',2: 'magenta',3:'red',4:'blue',5:'black',6: 'yellow',7: 'green', 8: "pink", 9: "purple", 10: "white", 11: "grey"}
fig, ax = plt.subplots()
ax.scatter(y, z, c=df_time['kmode'].map(colors))
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], z[i]))

df_bigger_than_600 = df_sui.loc[(df_sui['PassengerCount'] >= 300)]
df_bigger_than_600

df_huang_0 =  df_bigger_than_600[df_bigger_than_600.kmode == 0]
df_huang_1 =  df_bigger_than_600[df_bigger_than_600.kmode == 1]
df_huang_2 =  df_bigger_than_600[df_bigger_than_600.kmode == 2]
df_huang_3 =  df_bigger_than_600[df_bigger_than_600.kmode == 3]
df_huang_4 =  df_bigger_than_600[df_bigger_than_600.kmode == 4]
df_huang_5 =  df_bigger_than_600[df_bigger_than_600.kmode == 5]
df_huang_6 =  df_bigger_than_600[df_bigger_than_600.kmode == 6]

df_huangs = [df_huang_0,df_huang_1,df_huang_2,df_huang_3,df_huang_4,df_huang_5,df_huang_6]
count = 0
for x in df_huangs:
  print("-----")
  print("DF_HUANG_" + str(count))
  print(x.groupby("Line")["EncodedTime"].count())
  print(x.describe())
  count = count+1

df_huangs = [df_huang_0,df_huang_1,df_huang_2,df_huang_3,df_huang_4,df_huang_5,df_huang_6]
count = 0
for x in df_huangs:
  print("-----")
  print("DF_HUANG_" + str(count))
  print(x.groupby("EncodedTime")["Line"].count())
  print(x.describe())
  count = count+1

"""# Line Graph Visualization for Each Individual Bus Line - Passenger Count - Time"""

## Let's visualize first, kmode was not helpful for my task

df_time = df_time.drop(["birch", "kmode"], axis=1)

# Set a style for the plot
plt.style.use('ggplot')

# Make a plot
fig, ax = plt.subplots()

# Add lines to it
sns.lineplot(ax=ax, data=df_time, x="EncodedTime", y="PassengerCount", hue="Line", legend=None)

# Add the text--for each line, find the end, annotate it with a label, and
# adjust the chart axes so that everything fits on.
line_names = df_time.Line.unique()
for line, name in zip(ax.lines, line_names):
	y = line.get_ydata()[-1]
	x = line.get_xdata()[-1]
	if not np.isfinite(y):
	    y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float("nan"))
	if not np.isfinite(y) or not np.isfinite(x):
	    continue     
	text = ax.annotate(name,
		       xy=(x, y),
		       xytext=(0, 0),
		       color=line.get_color(),
		       xycoords=(ax.get_xaxis_transform(),
				 ax.get_yaxis_transform()),
		       textcoords="offset points")
	text_width = (text.get_window_extent(
	fig.canvas.get_renderer()).transformed(ax.transData.inverted()).width)
	if np.isfinite(text_width):
		ax.set_xlim(ax.get_xlim()[0], text.xy[0] + text_width * 1.05)

plt.tight_layout()
plt.show()

"""So, actually it is as we thought, time based. However, some behaviours are different and more popular than others. Let's use PassengerCount segmentation to evaluate them."""

df_sui

"""## Line Graph Visualization for Bus Lines (Passenger Count > 300) """

df_higher_300 =  df_sui.loc[(df_sui['PassengerCount'] >= 300)]

# Set a style for the plot
plt.style.use('ggplot')

# Make a plot
fig, ax = plt.subplots()

# Add lines to it
sns.lineplot(ax=ax, data=df_higher_300, x="EncodedTime", y="PassengerCount", hue="Line", legend=None)

# Add the text--for each line, find the end, annotate it with a label, and
# adjust the chart axes so that everything fits on.
line_names = df_higher_300.Line.unique()
for line, name in zip(ax.lines, line_names):
	y = line.get_ydata()[-1]
	x = line.get_xdata()[-1]
	if not np.isfinite(y):
	    y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float("nan"))
	if not np.isfinite(y) or not np.isfinite(x):
	    continue     

	text = ax.annotate(name,
		       xy=(x, y),
		       xytext=(0, 0),
		       color=line.get_color(),
		       xycoords=(ax.get_xaxis_transform(),
				 ax.get_yaxis_transform()),
		       textcoords="offset points")

plt.show()

line_pass_sum = df_sui.groupby("Line")["PassengerCount"].sum()

line_pass_sum.describe()

pass_higher_mean = line_pass_sum.to_frame('PassengerCount').reset_index()

pass_higher_mean =  pass_higher_mean.loc[(pass_higher_mean['PassengerCount'] >= 7000)]

pass_higher_mean

# Set a style for the plot
plt.style.use('ggplot')

# Make a plot
fig, ax = plt.subplots()

# Add lines to it
sns.lineplot(ax=ax, data=df_higher_300, x="EncodedTime", y="PassengerCount", hue="Line", legend=None)

# Add the text--for each line, find the end, annotate it with a label, and
# adjust the chart axes so that everything fits on.
line_names = pass_higher_mean.Line.unique()
for line, name in zip(ax.lines, line_names):
	y = line.get_ydata()[-1]
	x = line.get_xdata()[-1]
	if not np.isfinite(y):
	    y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float("nan"))
	if not np.isfinite(y) or not np.isfinite(x):
	    continue     

	text = ax.annotate(name,
		       xy=(x, y),
		       xytext=(0, 0),
		       color=line.get_color(),
		       xycoords=(ax.get_xaxis_transform(),
				 ax.get_yaxis_transform()),
		       textcoords="offset points")
	text_width = (text.get_window_extent(
	fig.canvas.get_renderer()).transformed(ax.transData.inverted()).width)
	
plt.show()

"""# Graph Similarity Calculation using Peak Fit"""

## OK, let's make some polynomial fit for these busy lines to find out the similar formula patterns and cluster them, then check out the minimal lines

## https://chrisostrouchov.com/post/peak_fit_xrd_python/

import math

vf01 = df.loc[(df['Line'] == "VF01")].reset_index()
vf01

!pip install lmfit 

from scipy import optimize, signal
from lmfit import models

def cost(parameters):
    a, b, c = parameters
    # y has been calculated in previous snippet
    return np.sum(np.power(g(x, a, b, c) - y, 2)) / len(x)

def print_best_values(spec, output):
    model_params = {
        'GaussianModel':   ['amplitude', 'sigma'],
        'LorentzianModel': ['amplitude', 'sigma'],
        'VoigtModel':      ['amplitude', 'sigma', 'gamma']
    }
    best_values = output.best_values
    print('center    model   amplitude     sigma      gamma')
    for i, model in enumerate(spec['model']):
        prefix = f'm{i}_'
        values = ', '.join(f'{best_values[prefix+param]:8.3f}' for param in model_params[model["type"]])
        print(f'[{best_values[prefix+"center"]:3.3f}] {model["type"]:16}: {values}')

import random

def generate_model(spec):
    composite_model = None
    params = None
    x = spec['x']
    y = spec['y']
    x_min = np.min(x)
    x_max = np.max(x)
    x_range = x_max - x_min
    y_max = np.max(y)
    for i, basis_func in enumerate(spec['model']):
        prefix = f'm{i}_'
        model = getattr(models, basis_func['type'])(prefix=prefix)
        if basis_func['type'] in ['GaussianModel', 'LorentzianModel', 'VoigtModel']: # for now VoigtModel has gamma constrained to sigma
            model.set_param_hint('sigma', min=1e-6, max=x_range)
            model.set_param_hint('center', min=x_min, max=x_max)
            model.set_param_hint('height', min=1e-6, max=1.1*y_max)
            model.set_param_hint('amplitude', min=1e-6)
            # default guess is horrible!! do not use guess()
            default_params = {
                prefix+'center': x_min + x_range * random.random(),
                prefix+'height': y_max * random.random(),
                prefix+'sigma': x_range * random.random()
            }
        else:
            raise NotImplemented(f'model {basis_func["type"]} not implemented yet')
        if 'help' in basis_func:  # allow override of settings in parameter
            for param, options in basis_func['help'].items():
                model.set_param_hint(param, **options)
        model_params = model.make_params(**default_params, **basis_func.get('params', {}))
        if params is None:
            params = model_params
        else:
            params.update(model_params)
        if composite_model is None:
            composite_model = model
        else:
            composite_model = composite_model + model
    return composite_model, params

def update_spec_from_peaks(spec, model_indicies, peak_widths=(10, 25), **kwargs):
    x = spec['x']
    y = spec['y']
    x_range = np.max(x) - np.min(x)
    peak_indicies = signal.find_peaks_cwt(y, peak_widths)
    np.random.shuffle(peak_indicies)
    for peak_indicie, model_indicie in zip(peak_indicies.tolist(), model_indicies):
        model = spec['model'][model_indicie]
        if model['type'] in ['GaussianModel', 'LorentzianModel', 'VoigtModel']:
            params = {
                'height': y[peak_indicie],
                'sigma': x_range / len(x) * np.min(peak_widths),
                'center': x[peak_indicie]
            }
            if 'params' in model:
                model.update(params)
            else:
                model['params'] = params
        else:
            raise NotImplemented(f'model {basis_func["type"]} not implemented yet')
    return peak_indicies

spec = {
    'x': vf01.index.values,
    'y': vf01['PassengerCount'].values,
    'model': [
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
    ]
}

peaks_found = update_spec_from_peaks(spec, [0, 1, 2, 3, 4, 5, 6], peak_widths=(15,))
fig, ax = plt.subplots()
ax.scatter(spec['x'], spec['y'], s=4)
for i in peaks_found:
    ax.axvline(x=spec['x'][i], c='black', linestyle='dotted')

model, params = generate_model(spec)
output = model.fit(spec['y'], params, x=spec['x'])
fig, gridspec = output.plot(xlabel="VF01",data_kws={'markersize':  1})





## OK THIS APPROACH MAY HELP A LOT TO CONFIRM SIMILARITY!

def save_best_values(spec, output, outputs, line):
    model_params = {
        'GaussianModel':   ['amplitude', 'sigma'],
        'LorentzianModel': ['amplitude', 'sigma'],
        'VoigtModel':      ['amplitude', 'sigma', 'gamma']
    }
    best_values = output.best_values
    print('center    model   amplitude     sigma      gamma')
    for i, model in enumerate(spec['model']):
        prefix = f'm{i}_'
        values = ', '.join(f'{best_values[prefix+param]:8.3f}' for param in model_params[model["type"]])
        outputs.append(line + f'- {best_values[prefix+"center"]:3.3f}- {model["type"]:16}: {values}')
        print(f'[{best_values[prefix+"center"]:3.3f}] {model["type"]:16}: {values}')

df

outputs = list()
line_names = df.Line.unique()
for line in line_names:
  print("--------")
  print(line)
  df_x = df.loc[(df['Line'] == line)].reset_index()
  spec = {
    'x': df_x.index.values,
    'y': df_x['PassengerCount'].values,
    'model': [
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'}]}

  peaks_found = update_spec_from_peaks(spec, [0, 1, 2, 3, 4, 5, 6], peak_widths=(15,))
  for i in peaks_found:
    model, params = generate_model(spec)
  
  output = model.fit(spec['y'], params, x=spec['x'])
  fig, gridspec = output.plot(xlabel=line + " Boarding Index", data_kws={'markersize':  1})
  save_best_values(spec, output, outputs, line)

outputs

## Now let's parse it.
## amplitude is coefficient and sigma is denominator of the power, which is actually std;

list_df = DataFrame (outputs,columns=['Raw'])

"""## Preprocessing the Outputs"""

list_df

list_df[['Line','Center', 'Model-Raw']] = list_df['Raw'].str.split('-',expand=True)

list_df

list_df[['amp', 'sigma', 'gamma']] = list_df['Model-Raw'].str.split(',',expand=True)

list_df

list_df[['Model', 'amp']] = list_df['amp'].str.split(':',expand=True)

list_df

col_to_drop = ["Raw", "Model-Raw", "gamma"]
list_df = list_df.drop(col_to_drop, axis=1)
list_df

## OK perfect. Now let's findout the similarity with a cost function we will build
scaler = StandardScaler()
le = preprocessing.LabelEncoder()
list_df['Model']= le.fit_transform(list_df['Model'])  

models_list = list()
stats_list = list()
line_list = list()

def type_builder(df):
  print(df)
  line = df.loc[0]["Line"]
  x = line + ","
  df = df.drop(["index","Line"], axis=1)
  df["Center"] = pd.to_numeric(df["Center"])
  df["amp"] = pd.to_numeric(df["amp"], downcast="float")
  df["sigma"] = pd.to_numeric(df["sigma"], downcast="float")
  df = df.sort_values(by=['Center'])

  for index, row in df.iterrows():
    x = x + " " +str(row["Model"])
  print(df)
  
  models_list.append(x)
  line_list.append(line) 
  y = df.describe().loc[['mean', 'std']]
  stats_list.append(y)

list_df

"""## Model Encoding"""

for line in line_names:
  print("--------")
  print(line)
  df_x = list_df.loc[(list_df['Line'] == line)].reset_index()
  type_builder(df_x)

models_list

"""## Grouping the Line Models"""

models_list = pd.DataFrame(models_list)
models_list[['Line', 'Model']] = models_list[0].str.split(',',expand=True)
models_list['Model'].nunique()

## OK nice, now we have 51 line and 39 distinct models. We also got the means and std of the line statistics. So we can check similarities

models_list.groupby("Model")["Line"].value_counts()

line_names = ["DC15", "KC06", "UC11"]
for line in line_names:
  print("--------")
  print(line)
  df_x = df.loc[(df['Line'] == line)].reset_index()
  spec = {
    'x': df_x.index.values,
    'y': df_x['PassengerCount'].values,
    'model': [
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'VoigtModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'},
        {'type': 'GaussianModel'}]}

  peaks_found = update_spec_from_peaks(spec, [0, 1, 2, 3, 4, 5, 6], peak_widths=(15,))
  for i in peaks_found:
    model, params = generate_model(spec)
  
  output = model.fit(spec['y'], params, x=spec['x'])
  fig, gridspec = output.plot(xlabel=line + " Boarding Index", data_kws={'markersize':  1})
  print_best_values(spec, output)

cols = ['1','2','3','4','5','6','7','8']

models_list = models_list.drop(0, axis=1)

models_list[cols] = models_list['Model'].str.split('.0 ',expand=True)
models_list

"""### Preprocessing Models"""

models_list[cols] = models_list[cols].apply(pd.to_numeric, errors='coerce', axis=1)
models_list

merged_models = pd.merge(models_list, df_line_encoded, on="Line")
col = ["Distance", "Start", "Terminal", "Start_X", "Start_Y","Mid_X", "Mid_Y","Terminal_X", "Terminal_Y",]
merged_models = merged_models.drop(col, axis=1)
merged_models

"""## Similar Behaviors Clustering using K-Prototypes (k=7)

K is chosen as 7 for heuristic approach. 2 or 5 could be also a choice according to elbow method; however, 7 has better groups according to my insights.
"""

km_huang = KModes(n_clusters=7, init = "Huang", n_init = 1, verbose=1)
fitClusters_huang = km_huang.fit_predict(merged_models)
merged_models["kmode"] = fitClusters_huang


z = merged_models["kmode"]
y = merged_models["PassengerCount"]
n = merged_models["Line"]

plt.rcParams["figure.figsize"] = (30,50)

colors = {0: 'yellowgreen',1: 'cyan',2: 'magenta',3:'red',4:'blue',5:'black',6: 'yellow',7: 'green', 8: "pink", 9: "purple", 10: "white", 11: "grey"}
fig, ax = plt.subplots()
ax.scatter(y, z, c=merged_models['kmode'].map(colors))
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], z[i]))

kmodes = merged_models.groupby("kmode")["Line"].value_counts()

kmodes

df_huang_0 =  merged_models[merged_models.kmode == 0]
df_huang_1 =  merged_models[merged_models.kmode == 1]
df_huang_2 =  merged_models[merged_models.kmode == 2]
df_huang_3 =  merged_models[merged_models.kmode == 3]
df_huang_4 =  merged_models[merged_models.kmode == 4]
df_huang_5 =  merged_models[merged_models.kmode == 5]
df_huang_6 =  merged_models[merged_models.kmode == 6]
df_huangs = [df_huang_0,df_huang_1,df_huang_2,df_huang_3,df_huang_4,df_huang_5,df_huang_6]

df_huang_0

df

"""## Similar Behaviors Visualizated in the Same Cluster Plots"""

def plotting(df, cluster):
  line_names = cluster.Line.unique()
  df_x = df[df['Line'].isin(line_names)]
  plt.style.use('ggplot')
  fig, ax = plt.subplots()
  sns.lineplot(ax=ax, data=df_x, x="EncodedTime", y="PassengerCount", hue="Line", legend=None)
  for line, name in zip(ax.lines, line_names):
    y = line.get_ydata()[-1]
    x = line.get_xdata()[-1]
    if not np.isfinite(y):
      y=next(reversed(line.get_ydata()[~line.get_ydata().mask]),float("nan"))
    if not np.isfinite(y) or not np.isfinite(x):
      continue     
    text = ax.annotate(name, xy=(x, y), 
                       xytext=(0, 0), color=line.get_color(),
                       xycoords=(ax.get_xaxis_transform(),
                                 ax.get_yaxis_transform()),
                       textcoords="offset points")
    text_width = (text.get_window_extent(
        fig.canvas.get_renderer()).transformed(ax.transData.inverted()).width)
    if np.isfinite(text_width):
      ax.set_xlim(ax.get_xlim()[0], text.xy[0] + text_width * 1.05)
    plt.tight_layout()
    plt.show()

count = 0
for x in df_huangs:
  print("-----")
  print("DF_HUANG_" + str(count))
  plotting(df,x)
  count = count+1

"""## Results:

The results are quite satisfactory as can be seen below. However, since clustering is unsupervised learning methodology, the results may vary in each run. Here is the latest result:

**Cluster 0:** KC35, MF40, MZ78, TB72, TC93, TCD49A, TL94, VC57, VC59, VF66, VML55A - These have 4 sharp arisen peaks during the day. Respectively morning, noon(around 12:00 and 15:00) and evening peaks are highly popular. But during the day, and especially after noon, the popularity is still higher than the usual. At 22:00, there is also a local maximum which are not often seen in other clusters. These lines are not the busiest routes, the average of the boarded passengers count is around 200. 

**Cluster 1:** AC03, DC15A, TC16, TCD49, VF01, VL13, VL13A - The peaks are mainly on morning and evening, one can easily say that passengers use these lines for business or education related reasons. These lines have a stable and continuously increasing line during the noon, which indicates these lines are preferred by passengers.

**Cluster 2:** 511, AF04, AF04A, CV67, KC33, KC35A, TCP45, UC11, UC32, VF63 - These lines have very frequent ridges and sharp edges during the day. Especially during the mid-day (10:00 - 15:00) they have increasing and multiple peaks. Also, at 22:00 we see another local maximum as well. The passenger count is lower than the other clusters' mean. Assuming these routes do not have alternative bus lines for their certain stops is logical.  

**Cluster 3:** CV48, KC06, KF52, KL08, LC07, LC07A, TK36, VC53 - These lines are the most popular ones during the day and night. One can say that these lines are the busiest and the most crowded ones after evening. There are multiple peaks during the day, as well. Therefore, assuming that these lines' bus stops are popular destinations is logical. 

**Cluster 4:**  DC15, GM24, LF10, MC12, VF02, VML54, VS18 - This cluster have different behavior during the pre-evening session. Between 15:00 and 19:00 there are 3 drastic peaks and then there is a dramatic loss of popularity afterwards. The peaks at 15:00 and 16:00 makes one think that students are the one who mainly uses these lines. 

**Cluster 5:**  CV14, CV47, KM61, LF09, ML22 -  The morning and pre-evening sessions' popularity is almost the same in these lines. During the day, one can see decrease between 09:00 - 12:00, then the plot is followed by sharp and increasing ridges. The main difference among the other clusters are seen at the evening session. There are Voigt-modeled peaks after 16:00, which indicates that these routes are preferred at evening and night session as well. 

**Cluster 6:**  FL82, KPZ83, TC16A - The morning peaks of other clusters are often lower than the evening peaks; however in this cluster passengers preferred these lines for morning routes. One can assume that these lines have alternative bus routes at evening due to loss of popularity after 16:00.
"""

merged_models.groupby("kmode")["Line"].value_counts()